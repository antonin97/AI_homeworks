{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70b07c13-5f80-4e28-9d90-e2391ff799fe",
   "metadata": {},
   "source": [
    "## Custom forward feet network\n",
    "created only with np library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb7f7610-ce23-4975-adde-abeb3565aaaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "aec57dc5-cabf-49bb-91fd-3d6fd510bbda",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN:\n",
    "    \n",
    "    def __init__(self, layers, activation, loss_function):\n",
    "        self.layers = layers\n",
    "        self.n_layers = len(layers)\n",
    "        self.n_weights = len(layers) - 1\n",
    "        \n",
    "        self.activations = {\n",
    "            \"relu\": self.relu,\n",
    "            \"sigmoid\": self.sigmoid,\n",
    "            \"tanh\": self.tanh\n",
    "        }\n",
    "\n",
    "        self.loss_functions = {\n",
    "            \"mse\": self.mse,\n",
    "            \"mae\": self.mae\n",
    "        }\n",
    "\n",
    "        self.activation_function = self.activations[activation]\n",
    "        self.loss_function = self.loss_functions[loss_function]\n",
    "\n",
    "        \n",
    "        self.weights = self.default_weights_init(layers)\n",
    "        self.biases = self.default_bias_init(layers)\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "    # gettting the weights from the normal distribution\n",
    "    def default_weights_init(self, layers):\n",
    "        weights = []\n",
    "        for i in range(self.n_weights):\n",
    "            weights.append(np.random.randn(layers[i], layers[i + 1]))\n",
    "        return weights\n",
    "\n",
    "    # setting all biases to zero\n",
    "    def default_bias_init(self, layers):\n",
    "        biases = []\n",
    "        for i in layers[1:]:\n",
    "            biases.append(np.random.randn(1, i))\n",
    "        return biases\n",
    "\n",
    "\n",
    "    # data set - array of tuples, where the first element is as input array and the second element is the desired output array\n",
    "    # ratios are the ratios of train, validation and test data\n",
    "    def load_data(self, X, y, ratios, categorization=False):\n",
    "        if categorization:\n",
    "            # one-hot encoding for the output layer - assumes indexing from 0 in the labels\n",
    "            num_cols = np.max(y) + 1\n",
    "            y = np.eye(num_cols)[y]\n",
    "        # checking if the data matches the NN architecture - I/O\n",
    "        #assert (len(X[0]) == self.layers[0]) and (len(y[0]) == self.layers[-1]), \"NN structure doesn't match the data\"\n",
    "        assert (sum(ratios) != 1), \"ratios should sum up to 1\"\n",
    "        idx1 = int(ratios[0] * len(X))\n",
    "        idx2 = int(sum(ratios[:2]) * len(X))\n",
    "        indices = np.arange(len(X))\n",
    "        np.random.shuffle(indices)\n",
    "        X_shuffled, y_shuffled = X[indices], y[indices]\n",
    "        self.X_train = X_shuffled[:idx1]\n",
    "        self.Y_train = y_shuffled[:idx1]\n",
    "        self.X_valid = X_shuffled[idx1:idx2]\n",
    "        self.Y_valid = y_shuffled[idx1:idx2]\n",
    "        self.X_test = X_shuffled[idx2:]\n",
    "        self.Y_test = y_shuffled[idx2:]\n",
    "\n",
    "    \n",
    "    def train_NN(self, n_epochs, batch_size, learning_rate):\n",
    "        for epoch in range(n_epochs + 1):\n",
    "            for _ in range (int(len(self.X_train / batch_size))):\n",
    "                # creating a minibatch on the fly\n",
    "                minibatch_indexes = np.random.choice(range(0, len(self.X_train)), batch_size, replace=False)\n",
    "                minibatch_X = self.X_train[minibatch_indexes]\n",
    "                minibatch_Y = self.Y_train[minibatch_indexes]\n",
    "                self.update_minibatch(minibatch_X, minibatch_Y, learning_rate, batch_size)\n",
    "            if epoch % 100 == 0:\n",
    "                self.evaluate_categorization(epoch)\n",
    "\n",
    "    # calculating gradients for a single minibatch + updating NN parameters\n",
    "    def update_minibatch(self, minibatch_X, minibatch_Y, learning_rate, batch_size):\n",
    "        nabla_weights = [np.zeros(w.shape) for w in self.weights]\n",
    "        nabla_biases = [np.zeros(b.shape) for b in self.biases]\n",
    "        for (X,Y) in zip(minibatch_X, minibatch_Y):\n",
    "            # backpropagation algorithm\n",
    "            delta_nabla_weights, delta_nabla_biases = self.backpropagation(X, Y)\n",
    "            # updating the nablas - adding gradients\n",
    "            nabla_weights = [nw + dnw for (nw, dnw) in zip(nabla_weights, delta_nabla_weights)]\n",
    "            nabla_biases = [nb + dnb for (nb, dnb) in zip(nabla_biases, delta_nabla_biases)]\n",
    "        # updating the NN parameters, averiging the gradients + multiplying by the learning rate\n",
    "        self.weights = [w - (learning_rate / batch_size) * delta_w for (w, delta_w) in zip(self.weights, nabla_weights)]\n",
    "        self.biases = [b - (learning_rate / batch_size) * delta_b for (b, delta_b) in zip(self.biases, nabla_biases)]\n",
    "\n",
    "\n",
    "    # expects input as an 1 x n  numpy array (n = neurons in input layer)\n",
    "    # only for evaluation, custom input to the network\n",
    "    def forward_pass(self, input):\n",
    "        for i in range(self.n_weights):\n",
    "            z = input @ self.weights[i] + self.biases[i]\n",
    "            activation = self.activation_function(z)\n",
    "            input = activation\n",
    "        return input\n",
    "\n",
    "    # backpropagation algorithm\n",
    "    def backpropagation(self, x, y):\n",
    "        # matrices of gradients - all zeros\n",
    "        nabla_weights = [np.zeros(w.shape) for w in self.weights]\n",
    "        nabla_biases = [np.zeros(b.shape) for b in self.biases]\n",
    "        # storing activations and z's for each layer\n",
    "        # storing the input as the first activation\n",
    "        activations = [x]\n",
    "        weighted_inputs = []\n",
    "\n",
    "        # feed forward + storing all information\n",
    "        for (w, b) in zip(self.weights, self.biases):\n",
    "            z = x @ w + b\n",
    "            x = self.activation_function(z)\n",
    "            # storing weighted input + activation for further use\n",
    "            weighted_inputs.append(z)\n",
    "            activations.append(x)\n",
    "\n",
    "        # backward pass\n",
    "        # error in the output layer\n",
    "        delta = (self.loss_function(activations[-1], y, derivation=True) * self.activation_function(weighted_inputs[-1], derivation=True))\n",
    "        # gradient of biases = error in the corresponding layer\n",
    "        nabla_biases[-1] = delta\n",
    "        # gradient of weights = error in the corresponding layer dotted/weighted with activations of the previous layer\n",
    "        nabla_weights[-1] = np.dot(activations[-2].T.reshape(-1, 1), delta)\n",
    "        # iterating layers second to last to the second (first one won't be updated)\n",
    "        for l in range(self.n_layers - 2, 0, -1):\n",
    "            current_z = weighted_inputs[l - 1]\n",
    "            current_z_der = self.activation_function(current_z, derivation=True)\n",
    "            #              1x2           2x4                      1x4\n",
    "            delta = np.dot(delta, self.weights[l].T) * current_z_der\n",
    "            nabla_biases[l - 1] = delta\n",
    "            nabla_weights[l - 1] = np.dot(activations[l - 1].T.reshape(-1, 1), delta)\n",
    "        \n",
    "        return (nabla_weights, nabla_biases)\n",
    "\n",
    "\n",
    "    # ------------------------------------- Evaluation Functions ------------------------------------- #\n",
    "\n",
    "    def evaluate(self):\n",
    "        for (X, y) in zip(self.X_valid, self.Y_valid):\n",
    "            y_hat = self.forward_pass(X)\n",
    "            error_validation = self.loss_function(y, y_hat)\n",
    "        for (X, y) in zip(self.X_train, self.Y_train):\n",
    "            y_hat = self.forward_pass(X)\n",
    "            error_training = self.loss_function(y, y_hat)\n",
    "        print(f\"train error: {error_training / len(self.X_train)} | validation error: {error_validation / len(self.X_valid)}\")\n",
    "\n",
    "    def evaluate_categorization(self, epoch):\n",
    "        y_hat = [np.argmax(self.forward_pass(X)) for X in self.X_valid] # NN assigned indexes\n",
    "        correct_to_validation = np.sum(y_hat == np.argmax(self.Y_valid, axis=1)) / len(self.X_valid)\n",
    "        y_hat = [np.argmax(self.forward_pass(X)) for X in self.X_train]\n",
    "        correct_to_training = np.sum(y_hat == np.argmax(self.Y_train, axis=1)) / len(self.X_train)\n",
    "        print(f\"epoch: {epoch:5} | train error: {1 - correct_to_training:.3f} | validation error: {1 - correct_to_validation:.3f}\")\n",
    "\n",
    "    # ------------------------------------- Activation Functions ------------------------------------- #\n",
    "    \n",
    "    # ReLU - 0 if x < 0 else x\n",
    "    def relu(self, input, derivation=False):\n",
    "        if not derivation:\n",
    "            return np.maximum(0, input)\n",
    "        else:\n",
    "            return np.where(input > 0, 1, 0)\n",
    "\n",
    "    # Sigmoid - 1 / (1 + e^-x)\n",
    "    def sigmoid(self, input, derivation=False):\n",
    "        if not derivation:\n",
    "            return 1 / (1 + np.exp(-input))\n",
    "        else:\n",
    "            return self.sigmoid(input)*(1-self.sigmoid(input))\n",
    "\n",
    "    # Tanh - 1 - 2 / (e^2x + 1)\n",
    "    def tanh(self, input, derivation=False):\n",
    "        if not derivation:\n",
    "            return 1 - 2 / (np.exp(2 * input) + 1)\n",
    "        else:\n",
    "            return 1 - self.tanh(input) ** 2\n",
    "            \n",
    "    \n",
    "    # ------------------------------------- Loss Functions ------------------------------------- #\n",
    "\n",
    "    # mean square error - classification loss function\n",
    "    # Y - true values that SHOULD be predicted by the model\n",
    "    # Y_hat - ACTUAL prediction by the model\n",
    "    def mse(self, y, y_hat, derivation=False):\n",
    "        if not derivation:\n",
    "            return 0.5 * (y - y_hat) ** 2\n",
    "        else:\n",
    "            return (y - y_hat)\n",
    "\n",
    "\n",
    "    # mean absolute error - classification loss function\n",
    "    # Y - true values that SHOULD be predicted by the model\n",
    "    # Y_hat - ACTUAL prediction by the model\n",
    "    def mae(self, y, y_hat, derivation=False):\n",
    "        if not derivation:\n",
    "            return 0.5 * (y - y_hat) ** 2\n",
    "        else:\n",
    "            return (y - y_hat)\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e0eabe-206c-4c68-98ba-1ece631ba050",
   "metadata": {},
   "source": [
    "### Iris dataset\n",
    "simple dataset for classifing iris species\\\n",
    "*Two of the three species were collected in the Gasp√© Peninsula \"all from the same pasture, and picked on the same day and measured at the same time by the same person with the same apparatus\".*\n",
    "\n",
    "data = [sepal.length, sepal.width, petal.length, petal.width] all floats\\\n",
    "labels = {0 : Setosa, 1: Versicolour, 2: Virginica}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "625dacf3-ba1c-4f64-84e3-7eb7d80e30a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "X_iris, y_iris = iris.data, iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "deee8506-1206-4cf2-b52b-f2cf2b6a9495",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_nn = NN([4, 4, 3], activation=\"tanh\", loss_function=\"mse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "46b99b03-459f-48e9-8f69-f85881dab236",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_nn.load_data(X_iris, y_iris, (0.7, 0.2, 0.1), categorization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "01e03934-0a9a-470c-8c38-2640ee2c3405",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:     0 | train error: 0.362 | validation error: 0.233\n",
      "epoch:   100 | train error: 0.352 | validation error: 0.267\n",
      "epoch:   200 | train error: 0.352 | validation error: 0.267\n",
      "epoch:   300 | train error: 0.362 | validation error: 0.233\n"
     ]
    }
   ],
   "source": [
    "iris_nn.train_NN(300, 16, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "34444711-28d2-4ca4-b424-2f939093a645",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(iris_nn.forward_pass([6. , 3. , 4.8, 1.8]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b48af117-d59e-47af-9d0e-6e5ab35addc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5.1 3.5 1.4 0.2] 0 0\n",
      "[4.9 3.  1.4 0.2] 0 0\n",
      "[4.7 3.2 1.3 0.2] 0 0\n",
      "[4.6 3.1 1.5 0.2] 0 0\n",
      "[5.  3.6 1.4 0.2] 0 0\n",
      "[5.4 3.9 1.7 0.4] 0 0\n",
      "[4.6 3.4 1.4 0.3] 0 0\n",
      "[5.  3.4 1.5 0.2] 0 0\n",
      "[4.4 2.9 1.4 0.2] 0 0\n",
      "[4.9 3.1 1.5 0.1] 0 0\n",
      "[5.4 3.7 1.5 0.2] 0 0\n",
      "[4.8 3.4 1.6 0.2] 0 0\n",
      "[4.8 3.  1.4 0.1] 0 0\n",
      "[4.3 3.  1.1 0.1] 0 0\n",
      "[5.8 4.  1.2 0.2] 0 0\n",
      "[5.7 4.4 1.5 0.4] 0 0\n",
      "[5.4 3.9 1.3 0.4] 0 0\n",
      "[5.1 3.5 1.4 0.3] 0 0\n",
      "[5.7 3.8 1.7 0.3] 0 0\n",
      "[5.1 3.8 1.5 0.3] 0 0\n",
      "[5.4 3.4 1.7 0.2] 0 0\n",
      "[5.1 3.7 1.5 0.4] 0 0\n",
      "[4.6 3.6 1.  0.2] 0 0\n",
      "[5.1 3.3 1.7 0.5] 0 0\n",
      "[4.8 3.4 1.9 0.2] 0 0\n",
      "[5.  3.  1.6 0.2] 0 0\n",
      "[5.  3.4 1.6 0.4] 0 0\n",
      "[5.2 3.5 1.5 0.2] 0 0\n",
      "[5.2 3.4 1.4 0.2] 0 0\n",
      "[4.7 3.2 1.6 0.2] 0 0\n",
      "[4.8 3.1 1.6 0.2] 0 0\n",
      "[5.4 3.4 1.5 0.4] 0 0\n",
      "[5.2 4.1 1.5 0.1] 0 0\n",
      "[5.5 4.2 1.4 0.2] 0 0\n",
      "[4.9 3.1 1.5 0.2] 0 0\n",
      "[5.  3.2 1.2 0.2] 0 0\n",
      "[5.5 3.5 1.3 0.2] 0 0\n",
      "[4.9 3.6 1.4 0.1] 0 0\n",
      "[4.4 3.  1.3 0.2] 0 0\n",
      "[5.1 3.4 1.5 0.2] 0 0\n",
      "[5.  3.5 1.3 0.3] 0 0\n",
      "[4.5 2.3 1.3 0.3] 0 0\n",
      "[4.4 3.2 1.3 0.2] 0 0\n",
      "[5.  3.5 1.6 0.6] 0 0\n",
      "[5.1 3.8 1.9 0.4] 0 0\n",
      "[4.8 3.  1.4 0.3] 0 0\n",
      "[5.1 3.8 1.6 0.2] 0 0\n",
      "[4.6 3.2 1.4 0.2] 0 0\n",
      "[5.3 3.7 1.5 0.2] 0 0\n",
      "[5.  3.3 1.4 0.2] 0 0\n",
      "[7.  3.2 4.7 1.4] 1 1\n",
      "[6.4 3.2 4.5 1.5] 1 1\n",
      "[6.9 3.1 4.9 1.5] 1 1\n",
      "[5.5 2.3 4.  1.3] 1 1\n",
      "[6.5 2.8 4.6 1.5] 1 1\n",
      "[5.7 2.8 4.5 1.3] 1 1\n",
      "[6.3 3.3 4.7 1.6] 1 1\n",
      "[4.9 2.4 3.3 1. ] 1 1\n",
      "[6.6 2.9 4.6 1.3] 1 1\n",
      "[5.2 2.7 3.9 1.4] 1 1\n",
      "[5.  2.  3.5 1. ] 1 1\n",
      "[5.9 3.  4.2 1.5] 1 1\n",
      "[6.  2.2 4.  1. ] 1 1\n",
      "[6.1 2.9 4.7 1.4] 1 1\n",
      "[5.6 2.9 3.6 1.3] 1 1\n",
      "[6.7 3.1 4.4 1.4] 1 1\n",
      "[5.6 3.  4.5 1.5] 1 1\n",
      "[5.8 2.7 4.1 1. ] 1 1\n",
      "[6.2 2.2 4.5 1.5] 1 1\n",
      "[5.6 2.5 3.9 1.1] 1 1\n",
      "[5.9 3.2 4.8 1.8] 1 2\n",
      "[6.1 2.8 4.  1.3] 1 1\n",
      "[6.3 2.5 4.9 1.5] 1 2\n",
      "[6.1 2.8 4.7 1.2] 1 1\n",
      "[6.4 2.9 4.3 1.3] 1 1\n",
      "[6.6 3.  4.4 1.4] 1 1\n",
      "[6.8 2.8 4.8 1.4] 1 1\n",
      "[6.7 3.  5.  1.7] 1 1\n",
      "[6.  2.9 4.5 1.5] 1 1\n",
      "[5.7 2.6 3.5 1. ] 1 1\n",
      "[5.5 2.4 3.8 1.1] 1 1\n",
      "[5.5 2.4 3.7 1. ] 1 1\n",
      "[5.8 2.7 3.9 1.2] 1 1\n",
      "[6.  2.7 5.1 1.6] 1 2\n",
      "[5.4 3.  4.5 1.5] 1 2\n",
      "[6.  3.4 4.5 1.6] 1 1\n",
      "[6.7 3.1 4.7 1.5] 1 1\n",
      "[6.3 2.3 4.4 1.3] 1 1\n",
      "[5.6 3.  4.1 1.3] 1 1\n",
      "[5.5 2.5 4.  1.3] 1 1\n",
      "[5.5 2.6 4.4 1.2] 1 1\n",
      "[6.1 3.  4.6 1.4] 1 1\n",
      "[5.8 2.6 4.  1.2] 1 1\n",
      "[5.  2.3 3.3 1. ] 1 1\n",
      "[5.6 2.7 4.2 1.3] 1 1\n",
      "[5.7 3.  4.2 1.2] 1 1\n",
      "[5.7 2.9 4.2 1.3] 1 1\n",
      "[6.2 2.9 4.3 1.3] 1 1\n",
      "[5.1 2.5 3.  1.1] 1 1\n",
      "[5.7 2.8 4.1 1.3] 1 1\n",
      "[6.3 3.3 6.  2.5] 2 2\n",
      "[5.8 2.7 5.1 1.9] 2 2\n",
      "[7.1 3.  5.9 2.1] 2 2\n",
      "[6.3 2.9 5.6 1.8] 2 2\n",
      "[6.5 3.  5.8 2.2] 2 2\n",
      "[7.6 3.  6.6 2.1] 2 2\n",
      "[4.9 2.5 4.5 1.7] 2 2\n",
      "[7.3 2.9 6.3 1.8] 2 2\n",
      "[6.7 2.5 5.8 1.8] 2 2\n",
      "[7.2 3.6 6.1 2.5] 2 2\n",
      "[6.5 3.2 5.1 2. ] 2 2\n",
      "[6.4 2.7 5.3 1.9] 2 2\n",
      "[6.8 3.  5.5 2.1] 2 2\n",
      "[5.7 2.5 5.  2. ] 2 2\n",
      "[5.8 2.8 5.1 2.4] 2 2\n",
      "[6.4 3.2 5.3 2.3] 2 2\n",
      "[6.5 3.  5.5 1.8] 2 2\n",
      "[7.7 3.8 6.7 2.2] 2 2\n",
      "[7.7 2.6 6.9 2.3] 2 2\n",
      "[6.  2.2 5.  1.5] 2 2\n",
      "[6.9 3.2 5.7 2.3] 2 2\n",
      "[5.6 2.8 4.9 2. ] 2 2\n",
      "[7.7 2.8 6.7 2. ] 2 2\n",
      "[6.3 2.7 4.9 1.8] 2 2\n",
      "[6.7 3.3 5.7 2.1] 2 2\n",
      "[7.2 3.2 6.  1.8] 2 2\n",
      "[6.2 2.8 4.8 1.8] 2 2\n",
      "[6.1 3.  4.9 1.8] 2 2\n",
      "[6.4 2.8 5.6 2.1] 2 2\n",
      "[7.2 3.  5.8 1.6] 2 2\n",
      "[7.4 2.8 6.1 1.9] 2 2\n",
      "[7.9 3.8 6.4 2. ] 2 2\n",
      "[6.4 2.8 5.6 2.2] 2 2\n",
      "[6.3 2.8 5.1 1.5] 2 2\n",
      "[6.1 2.6 5.6 1.4] 2 2\n",
      "[7.7 3.  6.1 2.3] 2 2\n",
      "[6.3 3.4 5.6 2.4] 2 2\n",
      "[6.4 3.1 5.5 1.8] 2 2\n",
      "[6.  3.  4.8 1.8] 2 2\n",
      "[6.9 3.1 5.4 2.1] 2 2\n",
      "[6.7 3.1 5.6 2.4] 2 2\n",
      "[6.9 3.1 5.1 2.3] 2 2\n",
      "[5.8 2.7 5.1 1.9] 2 2\n",
      "[6.8 3.2 5.9 2.3] 2 2\n",
      "[6.7 3.3 5.7 2.5] 2 2\n",
      "[6.7 3.  5.2 2.3] 2 2\n",
      "[6.3 2.5 5.  1.9] 2 2\n",
      "[6.5 3.  5.2 2. ] 2 2\n",
      "[6.2 3.4 5.4 2.3] 2 2\n",
      "[5.9 3.  5.1 1.8] 2 2\n"
     ]
    }
   ],
   "source": [
    "for x, y in zip(X_iris, y_iris):\n",
    "    print(x, y, np.argmax(example.forward_pass(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4073ad6c-4f85-44a6-9543-d03b8bc563d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
