{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb7f7610-ce23-4975-adde-abeb3565aaaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "aec57dc5-cabf-49bb-91fd-3d6fd510bbda",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN:\n",
    "    \n",
    "    def __init__(self, layers, activation):\n",
    "        self.layers = layers\n",
    "        self.n_layers = len(layers)\n",
    "        self.n_weights = len(layers) - 1\n",
    "        \n",
    "        self.activations = {\n",
    "            \"relu\": self.relu,\n",
    "            \"sigmoid\": self.sigmoid,\n",
    "            \"tanh\": self.tanh\n",
    "        }\n",
    "\n",
    "        self.activation_function = self.activations[activation]\n",
    "        \n",
    "        self.weights = self.default_weights_init(layers)\n",
    "        self.biases = self.default_bias_init(layers)\n",
    "        \n",
    "        self.loss_function = self.mse\n",
    "        self.loss_function_der = self.mse_der\n",
    "\n",
    "\n",
    "    # gettting the weights from the normal distribution\n",
    "    def default_weights_init(self, layers):\n",
    "        weights = []\n",
    "        for i in range(self.n_weights):\n",
    "            weights.append(np.random.randn(layers[i], layers[i + 1]))\n",
    "        return weights\n",
    "\n",
    "    # setting all biases to zero\n",
    "    def default_bias_init(self, layers):\n",
    "        biases = []\n",
    "        for i in layers[1:]:\n",
    "            biases.append(np.random.randn(1, i))\n",
    "        return biases\n",
    "\n",
    "\n",
    "    # data set - array of tuples, where the first element is as input array and the second element is the desired output array\n",
    "    # ratios are the ratios of train, validation and test data\n",
    "    def load_data(self, data, ratios):\n",
    "        # checking if the data matches the NN architecture - I/O\n",
    "        assert (len(data[0][0]) == self.layers[0]) and (len(data[0][1]) == self.layers[-1])\n",
    "        idx1 = int(ratios[0] * len(data))\n",
    "        idx2 = int(sum(ratios[:2]) * len(data))\n",
    "        np.random.shuffle(data)\n",
    "        self.X_train = np.array([t[0] for t in data[:idx1]])\n",
    "        self.Y_train = np.array([t[1] for t in data[:idx1]])\n",
    "        self.X_valid = np.array([t[0] for t in data[idx1:idx2]])\n",
    "        self.Y_valid = np.array([t[1] for t in data[idx1:idx2]])\n",
    "        self.X_test = np.array([t[0] for t in data[idx2:]])\n",
    "        self.Y_test = np.array([t[1] for t in data[idx2:]])\n",
    "\n",
    "    \n",
    "    def train_NN(self, n_epochs, batch_size, learning_rate):\n",
    "        for epoch in range(n_epochs):\n",
    "            for _ in range (int(len(self.X_train / batch_size))):\n",
    "                # creating a minibatch on the fly\n",
    "                minibatch_indexes = np.random.choice(range(0, len(self.X_train)), batch_size, replace=False)\n",
    "                minibatch_X = self.X_train[minibatch_indexes]\n",
    "                minibatch_Y = self.Y_train[minibatch_indexes]\n",
    "                self.update_minibatch(minibatch_X, minibatch_Y, learning_rate, batch_size)\n",
    "            if epoch % 100 == 0:\n",
    "                print(f\"epoch {epoch}: loss {self.evaluate()}\")\n",
    "\n",
    "    # calculating gradients for a single minibatch + updating NN parameters\n",
    "    def update_minibatch(self, minibatch_X, minibatch_Y, learning_rate, batch_size):\n",
    "        nabla_weights = [np.zeros(w.shape) for w in self.weights]\n",
    "        nabla_biases = [np.zeros(b.shape) for b in self.biases]\n",
    "        for (X,Y) in zip(minibatch_X, minibatch_Y):\n",
    "            # backpropagation algorithm\n",
    "            delta_nabla_weights, delta_nabla_biases = self.backpropagation(X, Y)\n",
    "            # updating the nablas - adding gradients\n",
    "            nabla_weights = [nw + dnw for (nw, dnw) in zip(nabla_weights, delta_nabla_weights)]\n",
    "            nabla_biases = [nb + dnb for (nb, dnb) in zip(nabla_biases, delta_nabla_biases)]\n",
    "        # updating the NN parameters, averiging the gradients + multiplying by the learning rate\n",
    "        self.weights = [w - (learning_rate / batch_size) * delta_w for (w, delta_w) in zip(self.weights, nabla_weights)]\n",
    "        self.biases = [b - (learning_rate / batch_size) * delta_b for (b, delta_b) in zip(self.biases, nabla_biases)]\n",
    "\n",
    "\n",
    "    # expects input as an 1 x n  numpy array (n = neurons in input layer)\n",
    "    # only for evaluation, custom input to the network\n",
    "    def forward_pass(self, input):\n",
    "        for i in range(self.n_weights):\n",
    "            z = input @ self.weights[i] + self.biases[i]\n",
    "            activation = self.activation_function(z)\n",
    "            input = activation\n",
    "        return input\n",
    "\n",
    "    # backpropagation algorithm\n",
    "    def backpropagation(self, x, y):\n",
    "        # matrices of gradients - all zeros\n",
    "        nabla_weights = [np.zeros(w.shape) for w in self.weights]\n",
    "        nabla_biases = [np.zeros(b.shape) for b in self.biases]\n",
    "        # storing activations and z's for each layer\n",
    "        # storing the input as the first activation\n",
    "        activations = [x]\n",
    "        weighted_inputs = []\n",
    "\n",
    "        # feed forward + storing all information\n",
    "        for (w, b) in zip(self.weights, self.biases):\n",
    "            z = x @ w + b\n",
    "            x = self.activation_function(z)\n",
    "            # storing weighted input + activation for further use\n",
    "            weighted_inputs.append(z)\n",
    "            activations.append(x)\n",
    "\n",
    "        # backward pass\n",
    "        # error in the output layer\n",
    "        delta = (self.loss_function_der(activations[-1], y) * self.activation_function(weighted_inputs[-1], derivation=True))\n",
    "        # gradient of biases = error in the corresponding layer\n",
    "        nabla_biases[-1] = delta\n",
    "        # gradient of weights = error in the corresponding layer dotted/weighted with activations of the previous layer\n",
    "        nabla_weights[-1] = np.dot(activations[-2].T.reshape(-1, 1), delta)\n",
    "        # iterating layers second to last to the second (first one won't be updated)\n",
    "        for l in range(self.n_layers - 2, 0, -1):\n",
    "            current_z = weighted_inputs[l - 1]\n",
    "            current_z_der = self.activation_function(current_z, derivation=True)\n",
    "            #              1x2           2x4                      1x4\n",
    "            delta = np.dot(delta, self.weights[l].T) * current_z_der\n",
    "            nabla_biases[l - 1] = delta\n",
    "            nabla_weights[l - 1] = np.dot(activations[l - 1].T.reshape(-1, 1), delta)\n",
    "        \n",
    "        return (nabla_weights, nabla_biases)\n",
    "\n",
    "\n",
    "    def evaluate(self):\n",
    "        n = 0\n",
    "        correct = 0\n",
    "        for (X, Y) in zip(self.X_valid, self.Y_valid):\n",
    "            n += 1\n",
    "            if np.round(self.forward_pass(X)) == Y:\n",
    "                correct += 1\n",
    "        return 1 - (correct / n)\n",
    "\n",
    "    # ------------------------------------- Activation Functions ------------------------------------- #\n",
    "    \n",
    "    # ReLU - 0 if x < 0 else x\n",
    "    def relu(self, input, derivation=False):\n",
    "        if not derivation:\n",
    "            return np.maximum(0, input)\n",
    "        else:\n",
    "            return np.where(input > 0, 1, 0)\n",
    "\n",
    "    # Sigmoid - 1 / (1 + e^-x)\n",
    "    def sigmoid(self, input, derivation=False):\n",
    "        if not derivation:\n",
    "            return 1 / (1 + np.exp(-input))\n",
    "        else:\n",
    "            return self.sigmoid(input)*(1-self.sigmoid(input))\n",
    "\n",
    "    # Tanh - 1 - 2 / (e^2x + 1)\n",
    "    def tanh(self, input, derivation=False):\n",
    "        if not derivation:\n",
    "            return 1 - 2 / (np.exp(2 * input) + 1)\n",
    "        else:\n",
    "            return 1 - self.tanh(input) ** 2\n",
    "            \n",
    "    \n",
    "    # ------------------------------------- Loss Functions ------------------------------------- #\n",
    "\n",
    "    # mean square error - categorization loss function\n",
    "    # Y - true values that SHOULD be predicted by the model\n",
    "    # Y_hat - ACTUAL prediction by the model\n",
    "    def mse(self, y, y_hat):\n",
    "        return 0.5 * (y - y_hat) ** 2\n",
    "\n",
    "    # mean square error derivative\n",
    "    def mse_der(self, y, y_hat):\n",
    "        return (y - y_hat)\n",
    "        \n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "deee8506-1206-4cf2-b52b-f2cf2b6a9495",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = NN([2, 3, 1], \"tanh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "46b99b03-459f-48e9-8f69-f85881dab236",
   "metadata": {},
   "outputs": [],
   "source": [
    "example.load_data(dummy, (0.7, 0.2, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "01e03934-0a9a-470c-8c38-2640ee2c3405",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0: loss 0.11111111111111116\n",
      "epoch 100: loss 0.0\n",
      "epoch 200: loss 0.0\n"
     ]
    }
   ],
   "source": [
    "example.train_NN(300, 12, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "34444711-28d2-4ca4-b424-2f939093a645",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.03863464]] 0\n",
      "[[0.98436839]] 1\n",
      "[[0.99018412]] 1\n",
      "[[0.03014505]] 0\n"
     ]
    }
   ],
   "source": [
    "print(example.forward_pass([1, 1]), 0)\n",
    "print(example.forward_pass([1, 0]), 1)\n",
    "print(example.forward_pass([0, 1]), 1)\n",
    "print(example.forward_pass([0, 0]), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c06de4b-1423-492b-914c-5ec069cefc06",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
