{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70b07c13-5f80-4e28-9d90-e2391ff799fe",
   "metadata": {},
   "source": [
    "## Custom forward feet network\n",
    "created only with np library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb7f7610-ce23-4975-adde-abeb3565aaaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "aec57dc5-cabf-49bb-91fd-3d6fd510bbda",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN:\n",
    "    \n",
    "    def __init__(self, layers, activation, loss_function):\n",
    "        self.layers = layers\n",
    "        self.n_layers = len(layers)\n",
    "        self.n_weights = len(layers) - 1\n",
    "        \n",
    "        self.activations = {\n",
    "            \"relu\": self.relu,\n",
    "            \"sigmoid\": self.sigmoid,\n",
    "            \"tanh\": self.tanh\n",
    "        }\n",
    "\n",
    "        self.loss_functions = {\n",
    "            \"mse\": self.mse,\n",
    "            \"mae\": self.mae\n",
    "        }\n",
    "\n",
    "        self.activation_function = self.activations[activation]\n",
    "        self.loss_function = self.loss_functions[loss_function]\n",
    "\n",
    "        \n",
    "        self.weights = self.default_weights_init(layers)\n",
    "        self.biases = self.default_bias_init(layers)\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "    # gettting the weights from the normal distribution\n",
    "    def default_weights_init(self, layers):\n",
    "        weights = []\n",
    "        for i in range(self.n_weights):\n",
    "            weights.append(np.random.randn(layers[i], layers[i + 1]))\n",
    "        return weights\n",
    "\n",
    "    # setting all biases to zero\n",
    "    def default_bias_init(self, layers):\n",
    "        biases = []\n",
    "        for i in layers[1:]:\n",
    "            biases.append(np.random.randn(1, i))\n",
    "        return biases\n",
    "\n",
    "\n",
    "    # data set - array of tuples, where the first element is as input array and the second element is the desired output array\n",
    "    # ratios are the ratios of train, validation and test data\n",
    "    def load_data(self, X, y, ratios, categorization=False):\n",
    "        if categorization:\n",
    "            # one-hot encoding for the output layer - assumes indexing from 0 in the labels\n",
    "            num_cols = np.max(y) + 1\n",
    "            y = np.eye(num_cols)[y]\n",
    "        # checking if the data matches the NN architecture - I/O\n",
    "        #assert (len(X[0]) == self.layers[0]) and (len(y[0]) == self.layers[-1]), \"NN structure doesn't match the data\"\n",
    "        assert (sum(ratios) != 1), \"ratios should sum up to 1\"\n",
    "        idx1 = int(ratios[0] * len(X))\n",
    "        idx2 = int(sum(ratios[:2]) * len(X))\n",
    "        indices = np.arange(len(X))\n",
    "        np.random.shuffle(indices)\n",
    "        X_shuffled, y_shuffled = X[indices], y[indices]\n",
    "        self.X_train = X_shuffled[:idx1]\n",
    "        self.Y_train = y_shuffled[:idx1]\n",
    "        self.X_valid = X_shuffled[idx1:idx2]\n",
    "        self.Y_valid = y_shuffled[idx1:idx2]\n",
    "        self.X_test = X_shuffled[idx2:]\n",
    "        self.Y_test = y_shuffled[idx2:]\n",
    "\n",
    "    \n",
    "    def train_NN(self, n_epochs, batch_size, learning_rate):\n",
    "        for epoch in range(n_epochs + 1):\n",
    "            for _ in range (int(len(self.X_train / batch_size))):\n",
    "                # creating a minibatch on the fly\n",
    "                minibatch_indexes = np.random.choice(range(0, len(self.X_train)), batch_size, replace=False)\n",
    "                minibatch_X = self.X_train[minibatch_indexes]\n",
    "                minibatch_Y = self.Y_train[minibatch_indexes]\n",
    "                self.update_minibatch(minibatch_X, minibatch_Y, learning_rate, batch_size)\n",
    "            if epoch % 100 == 0:\n",
    "                self.evaluate_categorization(epoch)\n",
    "\n",
    "    # calculating gradients for a single minibatch + updating NN parameters\n",
    "    def update_minibatch(self, minibatch_X, minibatch_Y, learning_rate, batch_size):\n",
    "        nabla_weights = [np.zeros(w.shape) for w in self.weights]\n",
    "        nabla_biases = [np.zeros(b.shape) for b in self.biases]\n",
    "        for (X,Y) in zip(minibatch_X, minibatch_Y):\n",
    "            # backpropagation algorithm\n",
    "            delta_nabla_weights, delta_nabla_biases = self.backpropagation(X, Y)\n",
    "            # updating the nablas - adding gradients\n",
    "            nabla_weights = [nw + dnw for (nw, dnw) in zip(nabla_weights, delta_nabla_weights)]\n",
    "            nabla_biases = [nb + dnb for (nb, dnb) in zip(nabla_biases, delta_nabla_biases)]\n",
    "        # updating the NN parameters, averiging the gradients + multiplying by the learning rate\n",
    "        self.weights = [w - (learning_rate / batch_size) * delta_w for (w, delta_w) in zip(self.weights, nabla_weights)]\n",
    "        self.biases = [b - (learning_rate / batch_size) * delta_b for (b, delta_b) in zip(self.biases, nabla_biases)]\n",
    "\n",
    "\n",
    "    # expects input as an 1 x n  numpy array (n = neurons in input layer)\n",
    "    # only for evaluation, custom input to the network\n",
    "    def forward_pass(self, input):\n",
    "        for i in range(self.n_weights):\n",
    "            z = input @ self.weights[i] + self.biases[i]\n",
    "            activation = self.activation_function(z)\n",
    "            input = activation\n",
    "        return input\n",
    "\n",
    "    # backpropagation algorithm\n",
    "    def backpropagation(self, x, y):\n",
    "        # matrices of gradients - all zeros\n",
    "        nabla_weights = [np.zeros(w.shape) for w in self.weights]\n",
    "        nabla_biases = [np.zeros(b.shape) for b in self.biases]\n",
    "        # storing activations and z's for each layer\n",
    "        # storing the input as the first activation\n",
    "        activations = [x]\n",
    "        weighted_inputs = []\n",
    "\n",
    "        # feed forward + storing all information\n",
    "        for (w, b) in zip(self.weights, self.biases):\n",
    "            z = x @ w + b\n",
    "            x = self.activation_function(z)\n",
    "            # storing weighted input + activation for further use\n",
    "            weighted_inputs.append(z)\n",
    "            activations.append(x)\n",
    "\n",
    "        # backward pass\n",
    "        # error in the output layer\n",
    "        delta = (self.loss_function(activations[-1], y, derivation=True) * self.activation_function(weighted_inputs[-1], derivation=True))\n",
    "        # gradient of biases = error in the corresponding layer\n",
    "        nabla_biases[-1] = delta\n",
    "        # gradient of weights = error in the corresponding layer dotted/weighted with activations of the previous layer\n",
    "        nabla_weights[-1] = np.dot(activations[-2].T.reshape(-1, 1), delta)\n",
    "        # iterating layers second to last to the second (first one won't be updated)\n",
    "        for l in range(self.n_layers - 2, 0, -1):\n",
    "            current_z = weighted_inputs[l - 1]\n",
    "            current_z_der = self.activation_function(current_z, derivation=True)\n",
    "            #              1x2           2x4                      1x4\n",
    "            delta = np.dot(delta, self.weights[l].T) * current_z_der\n",
    "            nabla_biases[l - 1] = delta\n",
    "            nabla_weights[l - 1] = np.dot(activations[l - 1].T.reshape(-1, 1), delta)\n",
    "        \n",
    "        return (nabla_weights, nabla_biases)\n",
    "\n",
    "\n",
    "    # ------------------------------------- Evaluation Functions ------------------------------------- #\n",
    "\n",
    "    def evaluate(self):\n",
    "        for (X, y) in zip(self.X_valid, self.Y_valid):\n",
    "            y_hat = self.forward_pass(X)\n",
    "            error_validation = self.loss_function(y, y_hat)\n",
    "        for (X, y) in zip(self.X_train, self.Y_train):\n",
    "            y_hat = self.forward_pass(X)\n",
    "            error_training = self.loss_function(y, y_hat)\n",
    "        print(f\"train error: {error_training / len(self.X_train)} | validation error: {error_validation / len(self.X_valid)}\")\n",
    "\n",
    "    def evaluate_categorization(self, epoch):\n",
    "        y_hat = [np.argmax(self.forward_pass(X)) for X in self.X_valid] # NN assigned indexes\n",
    "        correct_to_validation = np.sum(y_hat == np.argmax(self.Y_valid, axis=1)) / len(self.X_valid)\n",
    "        y_hat = [np.argmax(self.forward_pass(X)) for X in self.X_train]\n",
    "        correct_to_training = np.sum(y_hat == np.argmax(self.Y_train, axis=1)) / len(self.X_train)\n",
    "        print(f\"epoch: {epoch:5} | train error: {1 - correct_to_training:.3f} | validation error: {1 - correct_to_validation:.3f}\")\n",
    "\n",
    "    # ------------------------------------- Activation Functions ------------------------------------- #\n",
    "    \n",
    "    # ReLU - 0 if x < 0 else x\n",
    "    def relu(self, input, derivation=False):\n",
    "        if not derivation:\n",
    "            return np.maximum(0, input)\n",
    "        else:\n",
    "            return np.where(input > 0, 1, 0)\n",
    "\n",
    "    # Sigmoid - 1 / (1 + e^-x)\n",
    "    def sigmoid(self, input, derivation=False):\n",
    "        if not derivation:\n",
    "            return 1 / (1 + np.exp(-input))\n",
    "        else:\n",
    "            return self.sigmoid(input)*(1-self.sigmoid(input))\n",
    "\n",
    "    # Tanh - 1 - 2 / (e^2x + 1)\n",
    "    def tanh(self, input, derivation=False):\n",
    "        if not derivation:\n",
    "            return 1 - 2 / (np.exp(2 * input) + 1)\n",
    "        else:\n",
    "            return 1 - self.tanh(input) ** 2\n",
    "            \n",
    "    \n",
    "    # ------------------------------------- Loss Functions ------------------------------------- #\n",
    "\n",
    "    # mean square error - classification loss function\n",
    "    # Y - true values that SHOULD be predicted by the model\n",
    "    # Y_hat - ACTUAL prediction by the model\n",
    "    def mse(self, y, y_hat, derivation=False):\n",
    "        if not derivation:\n",
    "            return 0.5 * (y - y_hat) ** 2\n",
    "        else:\n",
    "            return (y - y_hat)\n",
    "\n",
    "\n",
    "    # mean absolute error - classification loss function\n",
    "    # Y - true values that SHOULD be predicted by the model\n",
    "    # Y_hat - ACTUAL prediction by the model\n",
    "    def mae(self, y, y_hat, derivation=False):\n",
    "        if not derivation:\n",
    "            return 0.5 * (y - y_hat) ** 2\n",
    "        else:\n",
    "            return (y - y_hat)\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e0eabe-206c-4c68-98ba-1ece631ba050",
   "metadata": {},
   "source": [
    "### Iris dataset\n",
    "simple dataset for classifing iris species\\\n",
    "*Two of the three species were collected in the Gaspé Peninsula \"all from the same pasture, and picked on the same day and measured at the same time by the same person with the same apparatus\".*\n",
    "\n",
    "data = [sepal.length, sepal.width, petal.length, petal.width] all floats\\\n",
    "labels = {0 : Setosa, 1: Versicolour, 2: Virginica}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "625dacf3-ba1c-4f64-84e3-7eb7d80e30a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "X_iris, y_iris = iris.data, iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "deee8506-1206-4cf2-b52b-f2cf2b6a9495",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_nn = NN([4, 4, 3], activation=\"tanh\", loss_function=\"mse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "46b99b03-459f-48e9-8f69-f85881dab236",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_nn.load_data(X_iris, y_iris, (0.7, 0.2, 0.1), categorization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "01e03934-0a9a-470c-8c38-2640ee2c3405",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:     0 | train error: 0.362 | validation error: 0.233\n",
      "epoch:   100 | train error: 0.352 | validation error: 0.267\n",
      "epoch:   200 | train error: 0.352 | validation error: 0.267\n",
      "epoch:   300 | train error: 0.362 | validation error: 0.233\n"
     ]
    }
   ],
   "source": [
    "iris_nn.train_NN(300, 16, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "34444711-28d2-4ca4-b424-2f939093a645",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(iris_nn.forward_pass([6. , 3. , 4.8, 1.8]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b48af117-d59e-47af-9d0e-6e5ab35addc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5.1 3.5 1.4 0.2] 0 0\n",
      "[4.9 3.  1.4 0.2] 0 0\n",
      "[4.7 3.2 1.3 0.2] 0 0\n",
      "[4.6 3.1 1.5 0.2] 0 0\n",
      "[5.  3.6 1.4 0.2] 0 0\n",
      "[5.4 3.9 1.7 0.4] 0 0\n",
      "[4.6 3.4 1.4 0.3] 0 0\n",
      "[5.  3.4 1.5 0.2] 0 0\n",
      "[4.4 2.9 1.4 0.2] 0 0\n",
      "[4.9 3.1 1.5 0.1] 0 0\n",
      "[5.4 3.7 1.5 0.2] 0 0\n",
      "[4.8 3.4 1.6 0.2] 0 0\n",
      "[4.8 3.  1.4 0.1] 0 0\n",
      "[4.3 3.  1.1 0.1] 0 0\n",
      "[5.8 4.  1.2 0.2] 0 0\n",
      "[5.7 4.4 1.5 0.4] 0 0\n",
      "[5.4 3.9 1.3 0.4] 0 0\n",
      "[5.1 3.5 1.4 0.3] 0 0\n",
      "[5.7 3.8 1.7 0.3] 0 0\n",
      "[5.1 3.8 1.5 0.3] 0 0\n",
      "[5.4 3.4 1.7 0.2] 0 0\n",
      "[5.1 3.7 1.5 0.4] 0 0\n",
      "[4.6 3.6 1.  0.2] 0 0\n",
      "[5.1 3.3 1.7 0.5] 0 0\n",
      "[4.8 3.4 1.9 0.2] 0 0\n",
      "[5.  3.  1.6 0.2] 0 0\n",
      "[5.  3.4 1.6 0.4] 0 0\n",
      "[5.2 3.5 1.5 0.2] 0 0\n",
      "[5.2 3.4 1.4 0.2] 0 0\n",
      "[4.7 3.2 1.6 0.2] 0 0\n",
      "[4.8 3.1 1.6 0.2] 0 0\n",
      "[5.4 3.4 1.5 0.4] 0 0\n",
      "[5.2 4.1 1.5 0.1] 0 0\n",
      "[5.5 4.2 1.4 0.2] 0 0\n",
      "[4.9 3.1 1.5 0.2] 0 0\n",
      "[5.  3.2 1.2 0.2] 0 0\n",
      "[5.5 3.5 1.3 0.2] 0 0\n",
      "[4.9 3.6 1.4 0.1] 0 0\n",
      "[4.4 3.  1.3 0.2] 0 0\n",
      "[5.1 3.4 1.5 0.2] 0 0\n",
      "[5.  3.5 1.3 0.3] 0 0\n",
      "[4.5 2.3 1.3 0.3] 0 0\n",
      "[4.4 3.2 1.3 0.2] 0 0\n",
      "[5.  3.5 1.6 0.6] 0 0\n",
      "[5.1 3.8 1.9 0.4] 0 0\n",
      "[4.8 3.  1.4 0.3] 0 0\n",
      "[5.1 3.8 1.6 0.2] 0 0\n",
      "[4.6 3.2 1.4 0.2] 0 0\n",
      "[5.3 3.7 1.5 0.2] 0 0\n",
      "[5.  3.3 1.4 0.2] 0 0\n",
      "[7.  3.2 4.7 1.4] 1 1\n",
      "[6.4 3.2 4.5 1.5] 1 1\n",
      "[6.9 3.1 4.9 1.5] 1 1\n",
      "[5.5 2.3 4.  1.3] 1 1\n",
      "[6.5 2.8 4.6 1.5] 1 1\n",
      "[5.7 2.8 4.5 1.3] 1 1\n",
      "[6.3 3.3 4.7 1.6] 1 1\n",
      "[4.9 2.4 3.3 1. ] 1 1\n",
      "[6.6 2.9 4.6 1.3] 1 1\n",
      "[5.2 2.7 3.9 1.4] 1 1\n",
      "[5.  2.  3.5 1. ] 1 1\n",
      "[5.9 3.  4.2 1.5] 1 1\n",
      "[6.  2.2 4.  1. ] 1 1\n",
      "[6.1 2.9 4.7 1.4] 1 1\n",
      "[5.6 2.9 3.6 1.3] 1 1\n",
      "[6.7 3.1 4.4 1.4] 1 1\n",
      "[5.6 3.  4.5 1.5] 1 1\n",
      "[5.8 2.7 4.1 1. ] 1 1\n",
      "[6.2 2.2 4.5 1.5] 1 1\n",
      "[5.6 2.5 3.9 1.1] 1 1\n",
      "[5.9 3.2 4.8 1.8] 1 2\n",
      "[6.1 2.8 4.  1.3] 1 1\n",
      "[6.3 2.5 4.9 1.5] 1 2\n",
      "[6.1 2.8 4.7 1.2] 1 1\n",
      "[6.4 2.9 4.3 1.3] 1 1\n",
      "[6.6 3.  4.4 1.4] 1 1\n",
      "[6.8 2.8 4.8 1.4] 1 1\n",
      "[6.7 3.  5.  1.7] 1 1\n",
      "[6.  2.9 4.5 1.5] 1 1\n",
      "[5.7 2.6 3.5 1. ] 1 1\n",
      "[5.5 2.4 3.8 1.1] 1 1\n",
      "[5.5 2.4 3.7 1. ] 1 1\n",
      "[5.8 2.7 3.9 1.2] 1 1\n",
      "[6.  2.7 5.1 1.6] 1 2\n",
      "[5.4 3.  4.5 1.5] 1 2\n",
      "[6.  3.4 4.5 1.6] 1 1\n",
      "[6.7 3.1 4.7 1.5] 1 1\n",
      "[6.3 2.3 4.4 1.3] 1 1\n",
      "[5.6 3.  4.1 1.3] 1 1\n",
      "[5.5 2.5 4.  1.3] 1 1\n",
      "[5.5 2.6 4.4 1.2] 1 1\n",
      "[6.1 3.  4.6 1.4] 1 1\n",
      "[5.8 2.6 4.  1.2] 1 1\n",
      "[5.  2.3 3.3 1. ] 1 1\n",
      "[5.6 2.7 4.2 1.3] 1 1\n",
      "[5.7 3.  4.2 1.2] 1 1\n",
      "[5.7 2.9 4.2 1.3] 1 1\n",
      "[6.2 2.9 4.3 1.3] 1 1\n",
      "[5.1 2.5 3.  1.1] 1 1\n",
      "[5.7 2.8 4.1 1.3] 1 1\n",
      "[6.3 3.3 6.  2.5] 2 2\n",
      "[5.8 2.7 5.1 1.9] 2 2\n",
      "[7.1 3.  5.9 2.1] 2 2\n",
      "[6.3 2.9 5.6 1.8] 2 2\n",
      "[6.5 3.  5.8 2.2] 2 2\n",
      "[7.6 3.  6.6 2.1] 2 2\n",
      "[4.9 2.5 4.5 1.7] 2 2\n",
      "[7.3 2.9 6.3 1.8] 2 2\n",
      "[6.7 2.5 5.8 1.8] 2 2\n",
      "[7.2 3.6 6.1 2.5] 2 2\n",
      "[6.5 3.2 5.1 2. ] 2 2\n",
      "[6.4 2.7 5.3 1.9] 2 2\n",
      "[6.8 3.  5.5 2.1] 2 2\n",
      "[5.7 2.5 5.  2. ] 2 2\n",
      "[5.8 2.8 5.1 2.4] 2 2\n",
      "[6.4 3.2 5.3 2.3] 2 2\n",
      "[6.5 3.  5.5 1.8] 2 2\n",
      "[7.7 3.8 6.7 2.2] 2 2\n",
      "[7.7 2.6 6.9 2.3] 2 2\n",
      "[6.  2.2 5.  1.5] 2 2\n",
      "[6.9 3.2 5.7 2.3] 2 2\n",
      "[5.6 2.8 4.9 2. ] 2 2\n",
      "[7.7 2.8 6.7 2. ] 2 2\n",
      "[6.3 2.7 4.9 1.8] 2 2\n",
      "[6.7 3.3 5.7 2.1] 2 2\n",
      "[7.2 3.2 6.  1.8] 2 2\n",
      "[6.2 2.8 4.8 1.8] 2 2\n",
      "[6.1 3.  4.9 1.8] 2 2\n",
      "[6.4 2.8 5.6 2.1] 2 2\n",
      "[7.2 3.  5.8 1.6] 2 2\n",
      "[7.4 2.8 6.1 1.9] 2 2\n",
      "[7.9 3.8 6.4 2. ] 2 2\n",
      "[6.4 2.8 5.6 2.2] 2 2\n",
      "[6.3 2.8 5.1 1.5] 2 2\n",
      "[6.1 2.6 5.6 1.4] 2 2\n",
      "[7.7 3.  6.1 2.3] 2 2\n",
      "[6.3 3.4 5.6 2.4] 2 2\n",
      "[6.4 3.1 5.5 1.8] 2 2\n",
      "[6.  3.  4.8 1.8] 2 2\n",
      "[6.9 3.1 5.4 2.1] 2 2\n",
      "[6.7 3.1 5.6 2.4] 2 2\n",
      "[6.9 3.1 5.1 2.3] 2 2\n",
      "[5.8 2.7 5.1 1.9] 2 2\n",
      "[6.8 3.2 5.9 2.3] 2 2\n",
      "[6.7 3.3 5.7 2.5] 2 2\n",
      "[6.7 3.  5.2 2.3] 2 2\n",
      "[6.3 2.5 5.  1.9] 2 2\n",
      "[6.5 3.  5.2 2. ] 2 2\n",
      "[6.2 3.4 5.4 2.3] 2 2\n",
      "[5.9 3.  5.1 1.8] 2 2\n"
     ]
    }
   ],
   "source": [
    "for x, y in zip(X_iris, y_iris):\n",
    "    print(x, y, np.argmax(example.forward_pass(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4073ad6c-4f85-44a6-9543-d03b8bc563d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
