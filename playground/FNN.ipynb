{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eb7f7610-ce23-4975-adde-abeb3565aaaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "aec57dc5-cabf-49bb-91fd-3d6fd510bbda",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN:\n",
    "    def __init__(self, layers):\n",
    "        self.layers = layers\n",
    "        self.n_layers = len(layers)\n",
    "        self.n_weights = len(layers) - 1\n",
    "        self.weights = self.default_weights_init(layers)\n",
    "        self.biases = self.default_bias_init(layers)\n",
    "        self.activation_function = self.sigmoid\n",
    "        self.activation_der = self.sigmoid_der\n",
    "        self.loss_function = self.mse\n",
    "        self.loss_function_der = self.mse_der\n",
    "\n",
    "\n",
    "    # gettting the weights from the normal distribution\n",
    "    # TODO - adding more clever methods of setting weights\n",
    "    def default_weights_init(self, layers):\n",
    "        weights = []\n",
    "        for i in range(self.n_weights):\n",
    "            weights.append(np.random.randn(layers[i], layers[i + 1]))\n",
    "        return weights\n",
    "\n",
    "    # setting all biases to zero\n",
    "    # TODO better method?\n",
    "    def default_bias_init(self, layers):\n",
    "        biases = []\n",
    "        for i in layers[1:]:\n",
    "            biases.append(np.zeros((1, i)))\n",
    "        return biases\n",
    "\n",
    "\n",
    "    # data set - array of tuples, where the first element is as input array and the second element is the desired output array\n",
    "    # ratios are the ratios of train, validation and test data\n",
    "    def load_data(self, data, ratios):\n",
    "        # checking if the data matches the NN architecture - I/O\n",
    "        assert (len(data[0][0]) == self.layers[0]) and (len(data[0][1]) == self.layers[-1])\n",
    "        idx1 = int(ratios[0] * len(data))\n",
    "        idx2 = int(sum(ratios[:2]) * len(data))\n",
    "        np.random.shuffle(data)\n",
    "        self.X_train = np.array([t[0] for t in data[:idx1]])\n",
    "        self.Y_train = np.array([t[1] for t in data[:idx1]])\n",
    "        self.X_valid = np.array([t[0] for t in data[idx1:idx2]])\n",
    "        self.Y_valid = np.array([t[1] for t in data[idx1:idx2]])\n",
    "        self.X_test = np.array([t[0] for t in data[idx2:]])\n",
    "        self.Y_test = np.array([t[1] for t in data[idx2:]])\n",
    "\n",
    "    \n",
    "    def train_NN(self, n_epochs, batch_size, learning_rate):\n",
    "        for epoch in range(n_epochs):\n",
    "            nabla_weights = [np.zeros(w.shape) for w in self.weights]\n",
    "            nabla_biases = [np.zeros(b.shape) for b in self.biases]\n",
    "\n",
    "            # nahradit loop tensorovým násobením\n",
    "            for _ in range(int(len(self.X_train) / batch_size)):\n",
    "                # creating a minibatch from the training data\n",
    "                minibatch_indexes = np.random.choice(range(0, len(self.X_train)), batch_size, replace=False)\n",
    "                minibatch_X = self.X_train[minibatch_indexes]\n",
    "                print(minibatch_X.shape)\n",
    "                minibatch_Y = self.Y_train[minibatch_indexes]\n",
    "                # backpropagation algorithm\n",
    "                delta_nabla_weights, delta_nabla_biases = self.backpropagation(minibatch_X, minibatch_Y)\n",
    "                # updating the nablas - adding gradients\n",
    "                nabla_weights = [nw + dnw for (nw, dnw) in zip(nabla_weights, delta_nabla_weights)]\n",
    "                nabla_biases = [nb + dnb for (nb, dnb) in zip(nabla_biases, delta_nabla_biases)]\n",
    "                \n",
    "            # updating the NN parameters, averiging the gradients + multiplying by the learning rate\n",
    "            self.weights = [w + (-learning_rate / batch_size) * delta_w for (w, delta_w) in zip(self.weights, nabla_weights)]\n",
    "            self.biases = [b + (-learning_rate / batch_size) * delta_b for (b, delta_b) in zip(self.biases, nabla_biases)]\n",
    "\n",
    "    \n",
    "    # expects input as an 1 x n  numpy array (n = neurons in input layer)\n",
    "    # only for evaluation, custom input to the network\n",
    "    def forward_pass(self, input):\n",
    "        for i in range(self.n_weights):\n",
    "            z = input @ self.weights[i] + self.biases[i]\n",
    "            activation = self.activation_function(z)\n",
    "            input = activation\n",
    "        return input\n",
    "\n",
    "    # backpropagation algorithm\n",
    "    def backpropagation(self, x, y):\n",
    "        # matrices of gradients - all zeros\n",
    "        nabla_weights = [np.zeros(w.shape) for w in self.weights]\n",
    "        nabla_biases = [np.zeros(b.shape) for b in self.biases]\n",
    "\n",
    "        # storing activations and z's for each layer\n",
    "        # storing the input as the first activation\n",
    "        activations, weighted_inputs = [x], []\n",
    "\n",
    "        # feed forward + storing all information\n",
    "        for (w, b) in zip(self.weights, self.biases):\n",
    "            z = x @ w + b\n",
    "            x = self.activation_function(z)\n",
    "            # storing weighted input + activation for further use\n",
    "            weighted_inputs.append(z)\n",
    "            activations.append(x)\n",
    "\n",
    "        # backward pass\n",
    "        # error in the output layer\n",
    "        delta = self.loss_function_der(y, activations[-1]) * self.activation_der(weighted_inputs[-1])\n",
    "        # gradient of biases = error in the corresponding layer\n",
    "        nabla_biases[-1] = delta\n",
    "        # gradient of weights = error in the corresponding layer dotted/weighted with activations of the previous layer\n",
    "        nabla_weights[-1] = np.dot(activations[-2].T, delta)\n",
    "\n",
    "        # iterating layers second to last to the second\n",
    "        for l in range(self.n_layers - 2, 0, -1):\n",
    "            current_z = weighted_inputs[l]\n",
    "            current_z_der = self.activation_der(current_z)\n",
    "            \n",
    "            delta = np.dot(delta, self.weights[l].T) * current_z_der\n",
    "            \n",
    "            nabla_biases[l] = delta\n",
    "            nabla_weights[l] = np.dot(activations[l - 1].T, delta)\n",
    "    \n",
    "\n",
    "        return (nabla_weights, nabla_biases)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    # ReLU - 0 if x < 0 else x\n",
    "    def relu(self, input):\n",
    "        return np.maximum(0, input)\n",
    "\n",
    "    \n",
    "    # sigmoid - 1 / (1 + e^-x)\n",
    "    def sigmoid(self, input):\n",
    "        return 1 / (1 + np.exp(-input))\n",
    "\n",
    "    # derivation of the sigmoid function\n",
    "    def sigmoid_der(self, z):\n",
    "        return self.sigmoid(z)*(1-self.sigmoid(z))\n",
    "\n",
    "\n",
    "    # mean square error - categorization loss function\n",
    "    # Y - true values that SHOULD be predicted by the model\n",
    "    # Y_hat - ACTUAL prediction by the model\n",
    "    def mse(self, y, y_hat):\n",
    "        return 0.5 * (y - y_hat) ** 2\n",
    "\n",
    "    # mean square error derivative\n",
    "    def mse_der(self, y, y_hat):\n",
    "        return (y - y_hat)\n",
    "        \n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "deee8506-1206-4cf2-b52b-f2cf2b6a9495",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = NN([3, 4, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "d0d3bf50-f7c8-45a2-a159-9b2690db9219",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy = [\n",
    "    ([7, 5, -3], [35]),\n",
    "    ([7, -8, -8], [4]),\n",
    "    ([6, 3, -4], [26]),\n",
    "    ([-1, -2, 4], [-4]),\n",
    "    ([6, 6, -6], [30]),\n",
    "    ([-5, 9, -9], [-11]),\n",
    "    ([-9, 4, 6], [-22]),\n",
    "    ([-8, -10, 1], [-51]),\n",
    "    ([-7, 8, -4], [-16]),\n",
    "    ([-1, -5, 6], [-8]),\n",
    "    ([-7, -10, -10], [-58]),\n",
    "    ([-10, 7, -4], [-30]),\n",
    "    ([-7, 7, -10], [-24]),\n",
    "    ([0, -5, -1], [-11]),\n",
    "    ([2, 8, -6], [18]),\n",
    "    ([2, -6, 5], [1]),\n",
    "    ([-10, -8, -8], [-64]),\n",
    "    ([3, 7, 8], [34]),\n",
    "    ([7, -1, -9], [17]),\n",
    "    ([-6, -10, -6], [-50])\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "46b99b03-459f-48e9-8f69-f85881dab236",
   "metadata": {},
   "outputs": [],
   "source": [
    "example.load_data(dummy, (0.8, 0.1, 0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "01e03934-0a9a-470c-8c38-2640ee2c3405",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 3)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (4,1) (3,4) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[293], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mexample\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_NN\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[289], line 62\u001b[0m, in \u001b[0;36mNN.train_NN\u001b[1;34m(self, n_epochs, batch_size, learning_rate)\u001b[0m\n\u001b[0;32m     60\u001b[0m     delta_nabla_weights, delta_nabla_biases \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackpropagation(minibatch_X, minibatch_Y)\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;66;03m# updating the nablas - adding gradients\u001b[39;00m\n\u001b[1;32m---> 62\u001b[0m     nabla_weights \u001b[38;5;241m=\u001b[39m [nw \u001b[38;5;241m+\u001b[39m dnw \u001b[38;5;28;01mfor\u001b[39;00m (nw, dnw) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(nabla_weights, delta_nabla_weights)]\n\u001b[0;32m     63\u001b[0m     nabla_biases \u001b[38;5;241m=\u001b[39m [nb \u001b[38;5;241m+\u001b[39m dnb \u001b[38;5;28;01mfor\u001b[39;00m (nb, dnb) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(nabla_biases, delta_nabla_biases)]\n\u001b[0;32m     65\u001b[0m \u001b[38;5;66;03m# updating the NN parameters, averiging the gradients + multiplying by the learning rate\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[289], line 62\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     60\u001b[0m     delta_nabla_weights, delta_nabla_biases \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackpropagation(minibatch_X, minibatch_Y)\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;66;03m# updating the nablas - adding gradients\u001b[39;00m\n\u001b[1;32m---> 62\u001b[0m     nabla_weights \u001b[38;5;241m=\u001b[39m [\u001b[43mnw\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdnw\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m (nw, dnw) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(nabla_weights, delta_nabla_weights)]\n\u001b[0;32m     63\u001b[0m     nabla_biases \u001b[38;5;241m=\u001b[39m [nb \u001b[38;5;241m+\u001b[39m dnb \u001b[38;5;28;01mfor\u001b[39;00m (nb, dnb) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(nabla_biases, delta_nabla_biases)]\n\u001b[0;32m     65\u001b[0m \u001b[38;5;66;03m# updating the NN parameters, averiging the gradients + multiplying by the learning rate\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (4,1) (3,4) "
     ]
    }
   ],
   "source": [
    "example.train_NN(1, 5, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "266b25fe-4795-481b-9ea3-4464ee200e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[1, 2, 4]])\n",
    "y = np.array([[9, 9, 9]])\n",
    "\n",
    "z = x * y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "c9e9a002-721a-4cce-8370-1acbccf2f0cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 9, 18, 36]])"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "974c7122-7638-46cf-be99-a7864fdc20d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 2, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "print(list(range(5 - 2, -1, -1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7372db70-8fcc-47a0-b273-6202c9bc6992",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
